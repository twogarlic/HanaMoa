import { OpenAI } from "openai";
import { getVectorStoreManager } from "./vectorStore";
import { Document } from "@langchain/core/documents";
import { PriceService } from "./priceService";
import { getCrossEncoderService } from "./crossEncoder";
import https from 'https';
import http from 'http';

// HTTP/2 ì—ì´ì „íŠ¸ ì„¤ì • 
const httpAgent = new http.Agent({
  keepAlive: true,
  keepAliveMsecs: 30000,
  maxSockets: 10,
  maxFreeSockets: 5,
  timeout: 60000,
});

const httpsAgent = new https.Agent({
  keepAlive: true,
  keepAliveMsecs: 30000,
  maxSockets: 10,
  maxFreeSockets: 5,
  timeout: 60000,
  rejectUnauthorized: true,
});

export class AdvancedRAGService {
  private client: OpenAI;
  private openaiClient: OpenAI;
  private vectorStoreManager: any;
  private model: string;
  private priceService: PriceService;
  private crossEncoderService: any;
  private connectionPool: OpenAI[] = [];
  private poolIndex: number = 0;
  private useLoRA: boolean;

  constructor() {
    this.client = new OpenAI({
      baseURL: "https://router.huggingface.co/v1",
      apiKey: process.env.HUGGINGFACE_API_KEY,
      // @ts-ignore - HTTP/2 ìµœì í™”ë¥¼ ìœ„í•œ ì—ì´ì „íŠ¸ ì„¤ì •
      httpAgent: httpsAgent,
    });
    this.model = process.env.HUGGINGFACE_MODEL || "MLP-KTLim/llama-3-Korean-Bllossom-8B:featherless-ai";
    
    // OpenAI API í´ë¼ì´ì–¸íŠ¸ 
    this.openaiClient = new OpenAI({
      apiKey: process.env.OPENAI_API_KEY,
      // @ts-ignore - HTTP/2 ìµœì í™”ë¥¼ ìœ„í•œ ì—ì´ì „íŠ¸ ì„¤ì •
      httpAgent: httpsAgent,
    });
    
    this.vectorStoreManager = getVectorStoreManager();
    this.priceService = new PriceService();
    this.crossEncoderService = getCrossEncoderService();
    this.useLoRA = process.env.USE_LORA_ADAPTER === "true";
    
    // LoRA ë³‘í•© ëª¨ë¸ ì‚¬ìš© 
    if (this.useLoRA) {
      this.model = "twogarlic/hana-moai-llama3-merged"; 
    }
    
    // ì—°ê²° í’€ë§: ë¯¸ë¦¬ 3ê°œì˜ HuggingFace í´ë¼ì´ì–¸íŠ¸ ìƒì„± (HTTP/2 ì§€ì›)
    for (let i = 0; i < 3; i++) {
      this.connectionPool.push(new OpenAI({
        baseURL: "https://router.huggingface.co/v1",
        apiKey: process.env.HUGGINGFACE_API_KEY,
        // @ts-ignore - HTTP/2 ìµœì í™”ë¥¼ ìœ„í•œ ì—ì´ì „íŠ¸ ì„¤ì •
        httpAgent: httpsAgent,
      }));
    }
  }

  // ì—°ê²° í’€ì—ì„œ ë‹¤ìŒ í´ë¼ì´ì–¸íŠ¸ ê°€ì ¸ì˜¤ê¸° 
  private getNextClient(): OpenAI {
    const client = this.connectionPool[this.poolIndex];
    this.poolIndex = (this.poolIndex + 1) % this.connectionPool.length;
    return client;
  }

  // HyDE: ê°€ìƒ ë¬¸ì„œ ìƒì„± í›„ ê²€ìƒ‰
  async generateHyDEQuery(userMessage: string): Promise<string> {
    try {
      const hydePrompt = `ë‹¹ì‹ ì€ í•˜ë‚˜ëª¨ì•„ ì„œë¹„ìŠ¤ì— ëŒ€í•´ ì˜ ì•Œê³  ìˆëŠ” ê¸ˆìœµ ì „ë¬¸ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì•„ë˜ ì§ˆë¬¸ì— ëŒ€í•´ í•˜ë‚˜ëª¨ì•„ ì„œë¹„ìŠ¤ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ,
ì „ë¬¸ì ì´ì§€ë§Œ ì´í•´í•˜ê¸° ì‰¬ìš´ ì„¤ëª…í˜• ë¬¸ì„œ(300~500ì)ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.

ìš”êµ¬ì‚¬í•­:
1. ì˜¤ì§ í•˜ë‚˜ëª¨ì•„ì˜ ì„œë¹„ìŠ¤ ì •ë³´ë§Œ í¬í•¨í•  ê²ƒ
2. ê¸ˆ, ì€, ì™¸í™˜ íˆ¬ì ê´€ë ¨ ì‹¤ì œ ê¸°ëŠ¥ ì¤‘ì‹¬ìœ¼ë¡œ ì„¤ëª…í•  ê²ƒ
3. ì‹¤ìš©ì ì´ë©° ê³ ê°ì´ ì°¸ê³ í•  ë§Œí•œ ë°©ì‹ìœ¼ë¡œ êµ¬ì²´ì ìœ¼ë¡œ ì„œìˆ í•  ê²ƒ
4. ê³¼ì¥ ì—†ì´, ì‚¬ì‹¤ì— ê¸°ë°˜í•œ ì¤‘ë¦½ì ì¸ í†¤ ìœ ì§€
5. ë¬¸ë‹¨ìœ¼ë¡œ ëœ ë‹µë³€ë¬¸ í•˜ë‚˜ë§Œ ì‘ì„± (ëª©ì°¨ë‚˜ í•­ëª© ì—†ì´)

ì§ˆë¬¸: ${userMessage}

ë‹µë³€ ë¬¸ì„œ:`;


      const response = await this.openaiClient.chat.completions.create({
        model: "gpt-4o-mini", 
        messages: [{ role: "user", content: hydePrompt }],
        max_tokens: 250, 
        temperature: 0.7
      });

      return response.choices[0]?.message?.content || "";
    } catch (error) {
      console.error('HyDE ìƒì„± ì˜¤ë¥˜:', error);
      return userMessage; 
    }
  }

  // HyDE ê²€ìƒ‰ 
  async searchWithHyDE(userMessage: string, k: number = 3): Promise<Document[]> {
    try {
      const hydeDocument = await this.generateHyDEQuery(userMessage);
      return await this.vectorStoreManager.searchSimilarDocuments(hydeDocument, k);
    } catch (error) {
      console.error('HyDE ê²€ìƒ‰ ì˜¤ë¥˜:', error);
      return [];
    }
  }

  // Multi-Query
  async generateMultiQueries(userMessage: string): Promise<string[]> {
    try {
      const multiQueryPrompt = `ì§ˆë¬¸ì„ 2ê°€ì§€ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ í‘œí˜„(ê° ì¤„ì— í•˜ë‚˜ì”©, ë²ˆí˜¸/ê¸°í˜¸ ì—†ì´):
${userMessage}`;

      const response = await this.openaiClient.chat.completions.create({
        model: "gpt-4o-mini",
        messages: [{ role: "user", content: multiQueryPrompt }],
        max_tokens: 100, 
        temperature: 0.5 
      });

      const content = response.choices[0]?.message?.content || "";
      const queries = content
        .split('\n')
        .filter(line => line.trim().length > 0)
        .map(line => line.replace(/^\d+\.\s*/, '').replace(/^[-*]\s*/, '').trim())
        .filter(line => line.length > 5)
        .slice(0, 2); 

      return queries.length > 0 ? queries : [userMessage];
    } catch (error) {
      console.error('Multi-Query ìƒì„± ì˜¤ë¥˜:', error);
      return [userMessage];
    }
  }

  // Multi-Query ê²€ìƒ‰ 
  async searchWithMultiQuery(userMessage: string, k: number = 3): Promise<Document[]> {
    try {
      const queries = await this.generateMultiQueries(userMessage);
      
      const allResultsArrays = await Promise.all(
        queries.map(query => this.vectorStoreManager.searchSimilarDocuments(query, k))
      );
      
      // ê²°ê³¼ í‰íƒ„í™”
      const allResults = allResultsArrays.flat();

      return allResults;
    } catch (error) {
      console.error('Multi-Query ê²€ìƒ‰ ì˜¤ë¥˜:', error);
      return [];
    }
  }

  // RRF
  reciprocalRankFusion(resultsA: Document[], resultsB: Document[], k: number = 60): Document[] {
    const docScores = new Map<string, { doc: Document; score: number }>();

    // ê²°ê³¼ A ì ìˆ˜ ê³„ì‚°
    resultsA.forEach((doc, index) => {
      const docId = this.getDocumentId(doc);
      const score = 1 / (k + index + 1);
      docScores.set(docId, { doc, score: (docScores.get(docId)?.score || 0) + score });
    });

    // ê²°ê³¼ B ì ìˆ˜ ê³„ì‚°
    resultsB.forEach((doc, index) => {
      const docId = this.getDocumentId(doc);
      const score = 1 / (k + index + 1);
      docScores.set(docId, { doc, score: (docScores.get(docId)?.score || 0) + score });
    });

    // ì ìˆ˜ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ ë¬¸ì„œ ë°˜í™˜
    return Array.from(docScores.values())
      .sort((a, b) => b.score - a.score)
      .slice(0, 10)
      .map(({ doc }) => doc);
  }

  // ë¬¸ì„œ ID ìƒì„±
  private getDocumentId(doc: Document): string {
    return doc.metadata?.id || doc.pageContent.substring(0, 100).replace(/\s+/g, '_');
  }

  // MMR
  async mmrDiversification(
    documents: Document[], 
    query: string, 
    lambda: number = 0.7, 
    k: number = 5
  ): Promise<Document[]> {
    if (documents.length <= k) return documents;

    const selected: Document[] = [];
    const remaining = [...documents];

    // ì²« ë²ˆì§¸ ë¬¸ì„œëŠ” ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ê²ƒ
    if (remaining.length > 0) {
      selected.push(remaining.shift()!);
    }

    while (selected.length < k && remaining.length > 0) {
      let bestDoc: Document | null = null;
      let bestScore = -Infinity;

      for (const doc of remaining) {
        // ê´€ë ¨ì„± ì ìˆ˜ 
        const relevanceScore = this.calculateRelevance(doc, query);
        
        // ë‹¤ì–‘ì„± ì ìˆ˜ 
        const diversityScore = Math.min(
          ...selected.map(selectedDoc => 
            this.calculateSimilarity(doc, selectedDoc)
          )
        );

        // MMR ì ìˆ˜ = Î» * ê´€ë ¨ì„± - (1-Î») * ë‹¤ì–‘ì„±
        const mmrScore = lambda * relevanceScore - (1 - lambda) * diversityScore;

        if (mmrScore > bestScore) {
          bestScore = mmrScore;
          bestDoc = doc;
        }
      }

      if (bestDoc) {
        selected.push(bestDoc);
        remaining.splice(remaining.indexOf(bestDoc), 1);
      } else {
        break;
      }
    }

    return selected;
  }

  // ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° 
  private calculateRelevance(doc: Document, query: string): number {
    const queryWords = query.toLowerCase().split(/\s+/);
    const docWords = doc.pageContent.toLowerCase().split(/\s+/);
    
    const matches = queryWords.filter(word => 
      docWords.some(docWord => docWord.includes(word) || word.includes(docWord))
    );
    
    return matches.length / queryWords.length;
  }

  // ë¬¸ì„œ ê°„ ìœ ì‚¬ë„ ê³„ì‚°
  private calculateSimilarity(doc1: Document, doc2: Document): number {
    const words1 = new Set(doc1.pageContent.toLowerCase().split(/\s+/));
    const words2 = new Set(doc2.pageContent.toLowerCase().split(/\s+/));
    
    const intersection = new Set([...words1].filter(x => words2.has(x)));
    const union = new Set([...words1, ...words2]);
    
    return intersection.size / union.size;
  }

  // Cross-Encoder ë¦¬ë­í‚¹ 
  async crossEncoderRerank(query: string, documents: Document[]): Promise<Document[]> {
    try {
      console.log('ğŸ¤– Cross-Encoder ë°°ì¹˜ ë¦¬ë­í‚¹ ì‹œì‘');
      
      // ë°°ì¹˜ ì²˜ë¦¬ ë°©ì‹ ì‚¬ìš© 
      const rankedDocs = await this.crossEncoderService.rerankDocumentsBatch(
        query, 
        documents, 
        3,  // topK
        10  // batchSize: 10ê°œì”© ë™ì‹œ ì²˜ë¦¬
      );
      
      rankedDocs.forEach((doc: Document, index: number) => {
        const score = doc.metadata?.crossEncoderScore;
        console.log(`  ${index + 1}. ì ìˆ˜: ${score ? score.toFixed(3) : 'N/A'} - ${doc.pageContent.substring(0, 100)}...`);
      });
      
      return rankedDocs;
    } catch (error) {
      
      const scoredDocs = documents.map(doc => {
        const score = this.calculateRelevance(doc, query);
        return { doc, score };
      });

      return scoredDocs
        .sort((a, b) => b.score - a.score)
        .slice(0, 3)
        .map(({ doc }) => doc);
    }
  }

  async generateAdvancedResponseStream(
    userMessage: string,
    history: Array<{ role: 'user' | 'assistant'; content: string }> = []
  ): Promise<Response> {
    const startTime = Date.now();
    const timings: { [key: string]: number } = {};
    
    try {
      const priceCheckStart = Date.now();
      const priceQuery = this.priceService.isPriceQuery(userMessage);
      if (priceQuery.isPrice && priceQuery.asset) {
        const priceData = await this.priceService.getRealTimePrice(priceQuery.asset);
        if (priceData) {
          const reply = this.priceService.formatPriceInfo(priceData, priceQuery.asset);
          return new Response(JSON.stringify({ reply }), {
            headers: { 'Content-Type': 'application/json' }
          });
        }
      }
      timings['0_ì‹œì„¸ì²´í¬'] = Date.now() - priceCheckStart;

      const parallelSearchStart = Date.now();
      
      const [hydeQuery, multiQueries] = await Promise.all([
        this.generateHyDEQuery(userMessage),
        this.generateMultiQueries(userMessage)
      ]);
      
      const allQueries = [hydeQuery, ...multiQueries];
      const allResults = await this.vectorStoreManager.searchSimilarDocumentsBatch(allQueries, 3);
      
      const hydeResults = allResults.slice(0, 3);
      const multiQueryResults = allResults.slice(3);
      
      timings['1-2_ë³‘ë ¬ê²€ìƒ‰'] = Date.now() - parallelSearchStart;

      const rrfStart = Date.now();
      const fusedResults = this.reciprocalRankFusion(hydeResults, multiQueryResults);
      timings['3_RRFìœµí•©'] = Date.now() - rrfStart;

      const mmrStart = Date.now();
      const diversifiedResults = await this.mmrDiversification(fusedResults, userMessage);
      timings['4_MMRë‹¤ì–‘í™”'] = Date.now() - mmrStart;

      const crossEncoderStart = Date.now();
      const finalResults = await this.crossEncoderRerank(userMessage, diversifiedResults);
      timings['5_CrossEncoder'] = Date.now() - crossEncoderStart;

      const contextStart = Date.now();
      const context = finalResults
        .map((doc: any) => doc.pageContent)
        .join('\n\n');
      timings['6_ì»¨í…ìŠ¤íŠ¸êµ¬ì„±'] = Date.now() - contextStart;

      const llmStart = Date.now();
      const response = await this.generateLLMResponseStream(userMessage, context, history);

      const totalTime = Date.now() - startTime;
      Object.entries(timings).forEach(([step, time]) => {
        const percentage = ((time / totalTime) * 100).toFixed(1);
      });

      return response;

    } catch (error) {
      const fallbackResults = await this.vectorStoreManager.searchSimilarDocuments(userMessage, 3);
      const fallbackContext = fallbackResults.map((doc: any) => doc.pageContent).join('\n\n');
      const reply = await this.generateLLMResponse(userMessage, fallbackContext, history);
      return new Response(JSON.stringify({ reply }), {
        headers: { 'Content-Type': 'application/json' }
      });
    }
  }

  async generateAdvancedResponse(
    userMessage: string,
    history: Array<{ role: 'user' | 'assistant'; content: string }> = []
  ): Promise<string> {
    const startTime = Date.now();
    const timings: { [key: string]: number } = {};
    
    try {
      const priceCheckStart = Date.now();
      const priceQuery = this.priceService.isPriceQuery(userMessage);
      if (priceQuery.isPrice && priceQuery.asset) {
        const priceData = await this.priceService.getRealTimePrice(priceQuery.asset);
        if (priceData) {
          const totalTime = Date.now() - startTime;
          return this.priceService.formatPriceInfo(priceData, priceQuery.asset);
        }
      }
      timings['0_ì‹œì„¸ì²´í¬'] = Date.now() - priceCheckStart;

      const parallelSearchStart = Date.now();
      const [hydeResults, multiQueryResults] = await Promise.all([
        this.searchWithHyDE(userMessage, 5),
        this.searchWithMultiQuery(userMessage, 5)
      ]);
      timings['1-2_ë³‘ë ¬ê²€ìƒ‰'] = Date.now() - parallelSearchStart;

      const rrfStart = Date.now();
      const fusedResults = this.reciprocalRankFusion(hydeResults, multiQueryResults);
      timings['3_RRFìœµí•©'] = Date.now() - rrfStart;

      const mmrStart = Date.now();
      const diversifiedResults = await this.mmrDiversification(fusedResults, userMessage);
      timings['4_MMRë‹¤ì–‘í™”'] = Date.now() - mmrStart;

      const crossEncoderStart = Date.now();
      const finalResults = await this.crossEncoderRerank(userMessage, diversifiedResults);
      timings['5_CrossEncoder'] = Date.now() - crossEncoderStart;

      const contextStart = Date.now();
      const context = finalResults
        .map((doc: any) => doc.pageContent)
        .join('\n\n');
      timings['6_ì»¨í…ìŠ¤íŠ¸êµ¬ì„±'] = Date.now() - contextStart;

      const llmStart = Date.now();
      const response = await this.generateLLMResponse(userMessage, context, history);
      timings['7_LLMì‘ë‹µ'] = Date.now() - llmStart;

      const totalTime = Date.now() - startTime;
      Object.entries(timings).forEach(([step, time]) => {
        const percentage = ((time / totalTime) * 100).toFixed(1);
      });
      return response;

    } catch (error) {
      const fallbackResults = await this.vectorStoreManager.searchSimilarDocuments(userMessage, 3);
      const fallbackContext = fallbackResults.map((doc: any) => doc.pageContent).join('\n\n');
      return await this.generateLLMResponse(userMessage, fallbackContext, history);
    }
  }

  private cleanMarkdown(text: string): string {
    return text
      .replace(/^#{1,6}\s+/gm, '')
      .replace(/###\s*/g, '')
      .replace(/##\s*/g, '')
      .replace(/\*\*(.*?)\*\*/g, '$1')
      .replace(/__(.*?)__/g, '$1')
      .replace(/\*(.*?)\*/g, '$1')
      .replace(/_(.*?)_/g, '$1')
      .replace(/```[\s\S]*?```/g, '')
      .replace(/`([^`]+)`/g, '$1')
      .replace(/\[([^\]]+)\]\([^)]+\)/g, '$1')
      .replace(/^[\s]*[-*+]\s+/gm, '')
      .replace(/^\d+\.\s+/gm, '')
      .replace(/^>\s*/gm, '')
      .replace(/^[-*_]{3,}$/gm, '')
      .replace(/\n{3,}/g, '\n\n')
      .trim();
  }

  private async generateLLMResponseStream(
    userMessage: string,
    context: string,
    history: Array<{ role: 'user' | 'assistant'; content: string }>
  ): Promise<Response> {
    const messages: Array<{ role: 'system' | 'user' | 'assistant'; content: string }> = [
      {
        role: "system" as const,
        content: `ë‹¹ì‹ ì€ í•˜ë‚˜ê¸ˆìœµê·¸ë£¹ì˜ ë””ì§€í„¸ ìì‚° í”Œë«í¼ 'í•˜ë‚˜ëª¨ì•„' ì „ë¬¸ ìƒë‹´ ì±—ë´‡ì…ë‹ˆë‹¤.
í•­ìƒ í•œêµ­ì–´ë¡œ, ì´ëª¨í‹°ì½˜ ì—†ì´, ì¹œê·¼í•˜ê³  ê³µì†í•˜ê²Œ ë‹µë³€í•©ë‹ˆë‹¤.

ì°¸ê³  ì •ë³´:
${context || "ê´€ë ¨ ì •ë³´ ì—†ìŒ"}

ì¤‘ìš” ê·œì¹™:
- ì°¸ê³  ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”
- ì •ë³´ê°€ ë¶ˆì¶©ë¶„í•˜ë©´ "ì •í™•í•œ ì •ë³´ë¥¼ ìœ„í•´ ë¬¸ì˜í•´ì£¼ì„¸ìš”"ë¼ê³  ì•ˆë‚´í•˜ì„¸ìš”
- "ê³ ê°ë‹˜" í˜¸ì¹­ì„ ì‚¬ìš©í•˜ê³  ì¡´ì¹­ì„ ìœ ì§€í•˜ì„¸ìš”
- ë‹µë³€ì„ ì™„ì „íˆ ë§ˆë¬´ë¦¬í•˜ì„¸ìš” (ì¤‘ê°„ì— ëŠì§€ ë§ˆì„¸ìš”)
- ì ˆëŒ€ë¡œ ë§ˆí¬ë‹¤ìš´ ë¬¸ë²•ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš” (ìƒµ, ë³„í‘œ, ëŒ€ì‹œ, ìˆ«ìì  ë“± ëª¨ë‘ ê¸ˆì§€)
- ì œëª©ì´ë‚˜ êµ¬ë¶„ì´ í•„ìš”í•˜ë©´ ì¤„ë°”ê¿ˆë§Œ ì‚¬ìš©í•˜ì„¸ìš”
- ë°˜ë“œì‹œ ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”`
      }
    ];

    const recentHistory = history.slice(-6);
    for (const turn of recentHistory) {
      if (turn.role === 'user') {
        messages.push({
          role: "user" as const,
          content: turn.content
        });
      } else {
        messages.push({
          role: "assistant" as const,
          content: turn.content
        });
      }
    }

    messages.push({
      role: "user" as const,
      content: userMessage
    });

    const client = this.getNextClient();
    const stream = await client.chat.completions.create({
      model: this.model,
      messages: messages,
      max_tokens: 800, 
      temperature: 0.7,
      stream: true, 
    });

    const encoder = new TextEncoder();
    const self = this;
    const readableStream = new ReadableStream({
      async start(controller) {
        try {
          let fullText = '';
          for await (const chunk of stream) {
            const content = chunk.choices[0]?.delta?.content || '';
            fullText += content;
            
            const cleanedContent = self.cleanMarkdown(content);
            
            const data = JSON.stringify({ 
              content: cleanedContent, 
              done: false 
            });
            controller.enqueue(encoder.encode(`data: ${data}\n\n`));
          }

          const cleanedFullText = self.cleanMarkdown(fullText);
          const finalData = JSON.stringify({ 
            content: '', 
            done: true,
            reply: cleanedFullText 
          });
          controller.enqueue(encoder.encode(`data: ${finalData}\n\n`));
          controller.close();
        } catch (error) {
          console.error('ìŠ¤íŠ¸ë¦¬ë° ì—ëŸ¬:', error);
          controller.error(error);
        }
      },
    });

    return new Response(readableStream, {
      headers: {
        'Content-Type': 'text/event-stream',
        'Cache-Control': 'no-cache',
        'Connection': 'keep-alive',
      },
    });
  }

  private async generateLLMResponse(
    userMessage: string,
    context: string,
    history: Array<{ role: 'user' | 'assistant'; content: string }>
  ): Promise<string> {
    const messages: Array<{ role: 'system' | 'user' | 'assistant'; content: string }> = [
      {
        role: "system" as const,
        content: `ë‹¹ì‹ ì€ í•˜ë‚˜ê¸ˆìœµê·¸ë£¹ì˜ ë””ì§€í„¸ ìì‚° í”Œë«í¼ 'í•˜ë‚˜ëª¨ì•„' ì „ë¬¸ ìƒë‹´ ì±—ë´‡ì…ë‹ˆë‹¤.
í•­ìƒ í•œêµ­ì–´ë¡œ, ì´ëª¨í‹°ì½˜ ì—†ì´, ì¹œê·¼í•˜ê³  ê³µì†í•˜ê²Œ ë‹µë³€í•©ë‹ˆë‹¤.

ì°¸ê³  ì •ë³´:
${context || "ê´€ë ¨ ì •ë³´ ì—†ìŒ"}

ì¤‘ìš” ê·œì¹™:
- ì°¸ê³  ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”
- ì •ë³´ê°€ ë¶ˆì¶©ë¶„í•˜ë©´ "ì •í™•í•œ ì •ë³´ë¥¼ ìœ„í•´ ë¬¸ì˜í•´ì£¼ì„¸ìš”"ë¼ê³  ì•ˆë‚´í•˜ì„¸ìš”
- "ê³ ê°ë‹˜" í˜¸ì¹­ì„ ì‚¬ìš©í•˜ê³  ì¡´ì¹­ì„ ìœ ì§€í•˜ì„¸ìš”
- ë‹µë³€ì„ ì™„ì „íˆ ë§ˆë¬´ë¦¬í•˜ì„¸ìš” (ì¤‘ê°„ì— ëŠì§€ ë§ˆì„¸ìš”)
- ì ˆëŒ€ë¡œ ë§ˆí¬ë‹¤ìš´ ë¬¸ë²•ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš” (ìƒµ, ë³„í‘œ, ëŒ€ì‹œ, ìˆ«ìì  ë“± ëª¨ë‘ ê¸ˆì§€)
- ì œëª©ì´ë‚˜ êµ¬ë¶„ì´ í•„ìš”í•˜ë©´ ì¤„ë°”ê¿ˆë§Œ ì‚¬ìš©í•˜ì„¸ìš”
- ë°˜ë“œì‹œ ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”`
      }
    ];

      const recentHistory = history.slice(-6);
      for (const turn of recentHistory) {
        if (turn.role === 'user') {
          messages.push({
            role: "user" as const,
            content: turn.content
          });
        } else {
          messages.push({
            role: "assistant" as const,
            content: turn.content
          });
        }
      }

      messages.push({
        role: "user" as const,
        content: userMessage
      });

    const client = this.getNextClient();
    const stream = await client.chat.completions.create({
      model: this.model,
      messages: messages,
      max_tokens: 800, 
      temperature: 0.7,
      stream: true, 
    });

    let rawResponse = '';
    for await (const chunk of stream) {
      const content = chunk.choices[0]?.delta?.content || '';
      rawResponse += content;
    }

    if (!rawResponse) {
      return 'ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.';
    }

    return this.cleanMarkdown(rawResponse);
  }

  async initializeRAG(): Promise<void> {
    try {
      await this.vectorStoreManager.initializeVectorStore();
      console.log('ê³ ê¸‰ RAG ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.');
    } catch (error) {
      console.error('ê³ ê¸‰ RAG ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜:', error);
      throw error;
    }
  }
}

let advancedRAGService: AdvancedRAGService | null = null;

export function getAdvancedRAGService(): AdvancedRAGService {
  if (!advancedRAGService) {
    advancedRAGService = new AdvancedRAGService();
  }
  return advancedRAGService;
}
